{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage Supervisé: TP2 - A-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import precision_score\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travail à effectuer\n",
    "Dans ce TP, nous expérimentons la création de réseaux de neurones artificiels (A-NN) avec divers paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du dataset MNIST\n",
    "X, Y = fetch_openml('mnist_784', return_X_y = True, as_frame = False) \n",
    "\n",
    "# 49000 Données pour l'entrainement\n",
    "training_data = X[:49000]\n",
    "training_target = Y[:49000]\n",
    "\n",
    "# Les autres données sont utilisées pour les tests\n",
    "test_data = X[49000:]\n",
    "test_target = Y[49000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe de l'image: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test de l'affichage et de la classe de l'image n°4\n",
    "plt.imshow(X[4].reshape(28, 28), cmap=plt.cm.gray_r,interpolation=\"nearest\")\n",
    "print(f\"Classe de l'image: {Y[4]}\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.51009628\n",
      "Iteration 2, loss = 1.16751047\n",
      "Iteration 3, loss = 0.89632935\n",
      "Iteration 4, loss = 0.72398084\n",
      "Iteration 5, loss = 0.61059320\n",
      "Iteration 6, loss = 0.52703117\n",
      "Iteration 7, loss = 0.46025772\n",
      "Iteration 8, loss = 0.40018730\n",
      "Iteration 9, loss = 0.35950189\n",
      "Iteration 10, loss = 0.33587466\n",
      "Iteration 11, loss = 0.30308672\n",
      "Iteration 12, loss = 0.27451445\n",
      "Iteration 13, loss = 0.25538484\n",
      "Iteration 14, loss = 0.23905837\n",
      "Iteration 15, loss = 0.22987177\n",
      "Iteration 16, loss = 0.21375895\n",
      "Iteration 17, loss = 0.20267413\n",
      "Iteration 18, loss = 0.19191688\n",
      "Iteration 19, loss = 0.18290359\n",
      "Iteration 20, loss = 0.18259778\n",
      "Iteration 21, loss = 0.17230423\n",
      "Iteration 22, loss = 0.16541816\n",
      "Iteration 23, loss = 0.16046825\n",
      "Iteration 24, loss = 0.16221343\n",
      "Iteration 25, loss = 0.15161846\n",
      "Iteration 26, loss = 0.14553881\n",
      "Iteration 27, loss = 0.14067155\n",
      "Iteration 28, loss = 0.13965612\n",
      "Iteration 29, loss = 0.13633025\n",
      "Iteration 30, loss = 0.13105706\n",
      "Iteration 31, loss = 0.13266309\n",
      "Iteration 32, loss = 0.12862347\n",
      "Iteration 33, loss = 0.12117612\n",
      "Iteration 34, loss = 0.12069753\n",
      "Iteration 35, loss = 0.12032288\n",
      "Iteration 36, loss = 0.11225886\n",
      "Iteration 37, loss = 0.11200795\n",
      "Iteration 38, loss = 0.11082671\n",
      "Iteration 39, loss = 0.10491565\n",
      "Iteration 40, loss = 0.10779547\n",
      "Iteration 41, loss = 0.10580493\n",
      "Iteration 42, loss = 0.10769113\n",
      "Iteration 43, loss = 0.10131170\n",
      "Iteration 44, loss = 0.09646352\n",
      "Iteration 45, loss = 0.09951339\n",
      "Iteration 46, loss = 0.09554881\n",
      "Iteration 47, loss = 0.09348675\n",
      "Iteration 48, loss = 0.09062971\n",
      "Iteration 49, loss = 0.08867874\n",
      "Iteration 50, loss = 0.08816906\n",
      "Iteration 51, loss = 0.08605644\n",
      "Iteration 52, loss = 0.08691398\n",
      "Iteration 53, loss = 0.08795203\n",
      "Iteration 54, loss = 0.08596814\n",
      "Iteration 55, loss = 0.08424493\n",
      "Iteration 56, loss = 0.08080524\n",
      "Iteration 57, loss = 0.07589892\n",
      "Iteration 58, loss = 0.07760654\n",
      "Iteration 59, loss = 0.07976321\n",
      "Iteration 60, loss = 0.07749796\n",
      "Iteration 61, loss = 0.07885238\n",
      "Iteration 62, loss = 0.07781386\n",
      "Iteration 63, loss = 0.07718224\n",
      "Iteration 64, loss = 0.07578606\n",
      "Iteration 65, loss = 0.07503499\n",
      "Iteration 66, loss = 0.07193987\n",
      "Iteration 67, loss = 0.06904036\n",
      "Iteration 68, loss = 0.07014365\n",
      "Iteration 69, loss = 0.07452806\n",
      "Iteration 70, loss = 0.06939387\n",
      "Iteration 71, loss = 0.06859147\n",
      "Iteration 72, loss = 0.06742145\n",
      "Iteration 73, loss = 0.06876743\n",
      "Iteration 74, loss = 0.06802946\n",
      "Iteration 75, loss = 0.07124978\n",
      "Iteration 76, loss = 0.06824809\n",
      "Iteration 77, loss = 0.06257061\n",
      "Iteration 78, loss = 0.06326556\n",
      "Iteration 79, loss = 0.06598595\n",
      "Iteration 80, loss = 0.06884570\n",
      "Iteration 81, loss = 0.06249388\n",
      "Iteration 82, loss = 0.06416857\n",
      "Iteration 83, loss = 0.06365314\n",
      "Iteration 84, loss = 0.06227903\n",
      "Iteration 85, loss = 0.06232393\n",
      "Iteration 86, loss = 0.06005092\n",
      "Iteration 87, loss = 0.06160511\n",
      "Iteration 88, loss = 0.06330985\n",
      "Iteration 89, loss = 0.06716990\n",
      "Iteration 90, loss = 0.05936639\n",
      "Iteration 91, loss = 0.06042265\n",
      "Iteration 92, loss = 0.06092573\n",
      "Iteration 93, loss = 0.06116683\n",
      "Iteration 94, loss = 0.06188784\n",
      "Iteration 95, loss = 0.06261670\n",
      "Iteration 96, loss = 0.06076186\n",
      "Iteration 97, loss = 0.05507934\n",
      "Iteration 98, loss = 0.05554199\n",
      "Iteration 99, loss = 0.05581039\n",
      "Iteration 100, loss = 0.05284175\n",
      "Iteration 101, loss = 0.05081015\n",
      "Iteration 102, loss = 0.05556193\n",
      "Iteration 103, loss = 0.05459848\n",
      "Iteration 104, loss = 0.05818525\n",
      "Iteration 105, loss = 0.05956811\n",
      "Iteration 106, loss = 0.06239033\n",
      "Iteration 107, loss = 0.06311225\n",
      "Iteration 108, loss = 0.05688701\n",
      "Iteration 109, loss = 0.05177702\n",
      "Iteration 110, loss = 0.05066883\n",
      "Iteration 111, loss = 0.05808012\n",
      "Iteration 112, loss = 0.05852234\n",
      "Iteration 113, loss = 0.05266271\n",
      "Iteration 114, loss = 0.05769512\n",
      "Iteration 115, loss = 0.05550842\n",
      "Iteration 116, loss = 0.05525473\n",
      "Iteration 117, loss = 0.05618865\n",
      "Iteration 118, loss = 0.05183413\n",
      "Iteration 119, loss = 0.04947037\n",
      "Iteration 120, loss = 0.05797432\n",
      "Iteration 121, loss = 0.05933083\n",
      "Iteration 122, loss = 0.05123938\n",
      "Iteration 123, loss = 0.04812991\n",
      "Iteration 124, loss = 0.04850610\n",
      "Iteration 125, loss = 0.05526306\n",
      "Iteration 126, loss = 0.05114246\n",
      "Iteration 127, loss = 0.06014883\n",
      "Iteration 128, loss = 0.04939941\n",
      "Iteration 129, loss = 0.05430780\n",
      "Iteration 130, loss = 0.05414433\n",
      "Iteration 131, loss = 0.05250849\n",
      "Iteration 132, loss = 0.05277442\n",
      "Iteration 133, loss = 0.04741431\n",
      "Iteration 134, loss = 0.05114680\n",
      "Iteration 135, loss = 0.04962810\n",
      "Iteration 136, loss = 0.05548106\n",
      "Iteration 137, loss = 0.05138029\n",
      "Iteration 138, loss = 0.05506182\n",
      "Iteration 139, loss = 0.04935033\n",
      "Iteration 140, loss = 0.04930489\n",
      "Iteration 141, loss = 0.05107218\n",
      "Iteration 142, loss = 0.04559353\n",
      "Iteration 143, loss = 0.04526572\n",
      "Iteration 144, loss = 0.04568537\n",
      "Iteration 145, loss = 0.04607362\n",
      "Iteration 146, loss = 0.05541721\n",
      "Iteration 147, loss = 0.04783367\n",
      "Iteration 148, loss = 0.04624554\n",
      "Iteration 149, loss = 0.04653263\n",
      "Iteration 150, loss = 0.05565878\n",
      "Iteration 151, loss = 0.04634559\n",
      "Iteration 152, loss = 0.04905049\n",
      "Iteration 153, loss = 0.05401416\n",
      "Iteration 154, loss = 0.05244152\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "# Modèles de classification\n",
    "\n",
    "# Modèle avec hidden_layer_sizes = (50), soit 1 couche cachée de 50 Neurones\n",
    "Classifier_50 = MLPClassifier(hidden_layer_sizes =  (50), verbose=True).fit(training_data, training_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected class: 6\n",
      "Predicted class of image n°4: 6\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Prédiction de l'image n°4 du test set avec le modèle entrainé\n",
    "expected4 = test_target[4]\n",
    "predicted4 = Classifier_50.predict(test_data)[4]\n",
    "print(f\"Expected class: {expected4}\")\n",
    "print(f\"Predicted class of image n°4: {predicted4}\")\n",
    "if expected4 == predicted4:\n",
    "    print(\"Success\")\n",
    "else:\n",
    "    print(\"Failure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision du modèle par label [0-9]: [0.96317551 0.97253219 0.95485437 0.93606018 0.94299517 0.95666667\n",
      " 0.95089937 0.95552561 0.90014065 0.91120608]\n"
     ]
    }
   ],
   "source": [
    "# Précision du model sur le test set\n",
    "accuracy = precision_score(test_target, Classifier_50.predict(test_data), average=None)\n",
    "\n",
    "print(f\"Précision du modèle par label [0-9]: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_layer_size = 10\n",
      "Accuracy = 0.9597619047619048\n",
      "hidden_layer_size = 20\n",
      "Accuracy = 0.958047619047619\n",
      "hidden_layer_size = 30\n",
      "Accuracy = 0.9521904761904761\n",
      "hidden_layer_size = 40\n",
      "Accuracy = 0.7046190476190476\n",
      "hidden_layer_size = 50\n",
      "Accuracy = 0.10985714285714286\n",
      "hidden_layer_size = 60\n",
      "Accuracy = 0.10985714285714286\n",
      "hidden_layer_size = 70\n",
      "Accuracy = 0.10985714285714286\n",
      "hidden_layer_size = 80\n",
      "Accuracy = 0.10985714285714286\n",
      "hidden_layer_size = 90\n",
      "Accuracy = 0.10985714285714286\n",
      "hidden_layer_size = 100\n",
      "Accuracy = 0.10985714285714286\n",
      "Max accuracy: 0.9597619047619048 for 10 hidden layers\n"
     ]
    }
   ],
   "source": [
    "# Variation de 10 à 100 couches cachées de 25 neurones chacune\n",
    "\n",
    "#max (n_hidden_layer, accuracy) tuple\n",
    "max_accuracy = (0,0)\n",
    "\n",
    "\n",
    "neuron_number = 25\n",
    "for i in range(10, 101, 10):\n",
    "    \n",
    "    hidden_layer_s = []\n",
    "    for _ in range (0, i):\n",
    "        hidden_layer_s.append(neuron_number)\n",
    "    \n",
    "    print(f\"hidden_layer_size = {len(hidden_layer_s)}\", flush=True)\n",
    "    Classifier_n = MLPClassifier(hidden_layer_sizes =  hidden_layer_s).fit(training_data, training_target)\n",
    "    accuracy = precision_score(test_target, Classifier_n.predict(test_data), average=\"micro\")\n",
    "    print(f\"Accuracy = {accuracy}\")\n",
    "    if(accuracy > max_accuracy[0]):\n",
    "        max_accuracy = (accuracy, i)\n",
    "print(f\"Max accuracy: {max_accuracy[0]} for {max_accuracy[1]} hidden layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pousons remarquer qu'avec plus de 30 couches cachées de 25 neurones, le model commence à sur-apprendre. Cela veut dire que le model ne reconnait pas les images, mais apprend le training set par coeur. Cela rend donc le modèle inutilisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de neurones par couche: 13\n",
      "\n",
      "Modèle n° 1\n",
      "hidden_layer_size = 1\n",
      "-------- Temps d'apprentissage: 44.75517010688782s --------\n",
      "\n",
      "\n",
      "Modèle n° 2\n",
      "hidden_layer_size = 3\n",
      "-------- Temps d'apprentissage: 131.2097761631012s --------\n",
      "\n",
      "\n",
      "Modèle n° 3\n",
      "hidden_layer_size = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Temps d'apprentissage: 155.15372109413147s --------\n",
      "\n",
      "\n",
      "Modèle n° 4\n",
      "hidden_layer_size = 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Temps d'apprentissage: 148.73188853263855s --------\n",
      "\n",
      "\n",
      "Modèle n° 5\n",
      "hidden_layer_size = 9\n",
      "-------- Temps d'apprentissage: 171.2417275905609s --------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 5 modèles de classification, avec [1 ; 10] couches cachées\n",
    "# chacune avec [10 ; 300] neurones (choisis aléatoirement) \n",
    "\n",
    "# Nombre de neurones par couche\n",
    "neuron_number = random.randint(10,300)\n",
    "print(f\"Nombre de neurones par couche: {neuron_number}\\n\")\n",
    "\n",
    "\n",
    "# Génération de 5 modèles\n",
    "Classifier_models = []\n",
    "for i in range(0,10, 2):\n",
    "    print(f\"Modèle n° {round(i/2 + 1)}\", flush=True)\n",
    "    hidden_layer_s = []\n",
    "    for j in range (0, i+1):\n",
    "        hidden_layer_s.append(neuron_number)\n",
    "    print(f\"hidden_layer_size = {len(hidden_layer_s)}\", flush=True)\n",
    "    \n",
    "    ##### Temps d'apprentissage ####\n",
    "    start_time = time.time()\n",
    "    Classifier_n = MLPClassifier(hidden_layer_sizes =  hidden_layer_s).fit(training_data, training_target)\n",
    "    print(f\"-------- Temps d'apprentissage: {time.time() - start_time}s --------\")\n",
    "    ###############\n",
    "    \n",
    "    accuracy = precision_score(test_target, Classifier_n.predict(test_data), average=\"micro\")\n",
    "    Classifier_models.append(Classifier_n)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Modèle 1 ---\n",
      "Accuracy par label [0-9]: [0.9275225  0.94317218 0.90696542 0.83778715 0.90557509 0.80683584\n",
      " 0.87294333 0.93504673 0.82195007 0.87576065]\n",
      "Accuracy moyenne: 0.8833558951907392\n",
      "----------------\n",
      "\n",
      "--- Modèle 2 ---\n",
      "Accuracy par label [0-9]: [0.93723653 0.97649108 0.91654466 0.89881235 0.93310208 0.8828125\n",
      " 0.95402299 0.95774648 0.87092329 0.87592251]\n",
      "Accuracy moyenne: 0.9203614463549521\n",
      "----------------\n",
      "\n",
      "--- Modèle 3 ---\n",
      "Accuracy par label [0-9]: [0.96416627 0.95197978 0.93650047 0.91121076 0.93460621 0.94687327\n",
      " 0.96658354 0.95392336 0.92166344 0.93482587]\n",
      "Accuracy moyenne: 0.9422332970436269\n",
      "----------------\n",
      "\n",
      "--- Modèle 4 ---\n",
      "Accuracy par label [0-9]: [0.94228062 0.97963701 0.9088385  0.92741935 0.93288591 0.88634047\n",
      " 0.95982143 0.93512304 0.90760346 0.93533605]\n",
      "Accuracy moyenne: 0.9315285842262255\n",
      "----------------\n",
      "\n",
      "--- Modèle 5 ---\n",
      "Accuracy par label [0-9]: [0.93889925 0.97595103 0.93482398 0.90383726 0.94308145 0.92887473\n",
      " 0.93689788 0.9588015  0.89655172 0.9024976 ]\n",
      "Accuracy moyenne: 0.9320216409782841\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of the 5 models\n",
    "i = 1\n",
    "for Classifier in Classifier_models:\n",
    "    print(f\"--- Modèle {i} ---\")\n",
    "    accuracy = precision_score(test_target, Classifier.predict(test_data), average=None)\n",
    "    print(f\"Accuracy par label [0-9]: {accuracy}\")\n",
    "    print(f\"Accuracy moyenne: {statistics.mean(accuracy)}\")\n",
    "    print(\"----------------\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que le modèle ayant la meilleure accuracy est le modèle est le Modèle 3, avec 5 couches cachées. Cependant, si l'on fait un rapport temps d'apprentissage / performance, le meilleur modèle est le Modèle 1, avec une seule couche cachée (~ 3x moins de temps d'apprentissage que le modèle 3, pour environ 1.06x moins d'accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de neurones par layer: 175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comparaison des différents algorithmes d'optimisation\n",
    "\n",
    "# Création des paramètres des modèles\n",
    "\n",
    "# Séléction du nombre de neurones par couche\n",
    "neuron_number = random.randint(10,300)\n",
    "print(f\"Nombre de neurones par layer: {neuron_number}\\n\")\n",
    "\n",
    "# Remplissage des layers\n",
    "layer_number = 10\n",
    "hidden_layer_s = []\n",
    "for i in range (0, layer_number+1):\n",
    "    hidden_layer_s.append(neuron_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_times = [0,0,0] # Temps d'entrainements des classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L-BFSG Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pierre/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "print(\"L-BFSG Classifier\")\n",
    "start_time = time.time()\n",
    "Classifier_SGD = MLPClassifier(hidden_layer_sizes =  hidden_layer_s, solver = \"lbfgs\", verbose=True).fit(training_data, training_target)\n",
    "training_times[0] = time.time()-start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le classifier BFSG n'a pas réussit à converger après 200 itérations. L'entrainement est donc stoppé automatiquement pour ne pas avoir un temps d'entrainement excessif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Classifier\n",
      "Iteration 1, loss = 0.59714656\n",
      "Iteration 2, loss = 0.21687473\n",
      "Iteration 3, loss = 0.15620524\n",
      "Iteration 4, loss = 0.12329669\n",
      "Iteration 5, loss = 0.09890756\n",
      "Iteration 6, loss = 0.08519185\n",
      "Iteration 7, loss = 0.06954327\n",
      "Iteration 8, loss = 0.05926144\n",
      "Iteration 9, loss = 0.05022173\n",
      "Iteration 10, loss = 0.04091124\n",
      "Iteration 11, loss = 0.03381930\n",
      "Iteration 12, loss = 0.02867593\n",
      "Iteration 13, loss = 0.02347707\n",
      "Iteration 14, loss = 0.01843724\n",
      "Iteration 15, loss = 0.01580756\n",
      "Iteration 16, loss = 0.01207078\n",
      "Iteration 17, loss = 0.00834927\n",
      "Iteration 18, loss = 0.00558848\n",
      "Iteration 19, loss = 0.00455299\n",
      "Iteration 20, loss = 0.00412005\n",
      "Iteration 21, loss = 0.00259151\n",
      "Iteration 22, loss = 0.00247025\n",
      "Iteration 23, loss = 0.00184347\n",
      "Iteration 24, loss = 0.00151774\n",
      "Iteration 25, loss = 0.00131777\n",
      "Iteration 26, loss = 0.00120311\n",
      "Iteration 27, loss = 0.00111151\n",
      "Iteration 28, loss = 0.00107432\n",
      "Iteration 29, loss = 0.00101641\n",
      "Iteration 30, loss = 0.00096908\n",
      "Iteration 31, loss = 0.00093640\n",
      "Iteration 32, loss = 0.00090829\n",
      "Iteration 33, loss = 0.00088444\n",
      "Iteration 34, loss = 0.00086058\n",
      "Iteration 35, loss = 0.00084072\n",
      "Iteration 36, loss = 0.00081926\n",
      "Iteration 37, loss = 0.00080401\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "print(\"SGD Classifier\")\n",
    "start_time = time.time()\n",
    "Classifier_SGD = MLPClassifier(hidden_layer_sizes =  hidden_layer_s, solver = \"sgd\", verbose=True).fit(training_data, training_target)\n",
    "training_times[1] = time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADAM Classifier\n",
      "Iteration 1, loss = 0.43078210\n",
      "Iteration 2, loss = 0.15306790\n",
      "Iteration 3, loss = 0.11354987\n",
      "Iteration 4, loss = 0.09171630\n",
      "Iteration 5, loss = 0.08023250\n",
      "Iteration 6, loss = 0.06917666\n",
      "Iteration 7, loss = 0.06161239\n",
      "Iteration 8, loss = 0.05692381\n",
      "Iteration 9, loss = 0.04756749\n",
      "Iteration 10, loss = 0.04366743\n",
      "Iteration 11, loss = 0.04102061\n",
      "Iteration 12, loss = 0.04535244\n",
      "Iteration 13, loss = 0.03911469\n",
      "Iteration 14, loss = 0.03645076\n",
      "Iteration 15, loss = 0.02917702\n",
      "Iteration 16, loss = 0.03654315\n",
      "Iteration 17, loss = 0.03598135\n",
      "Iteration 18, loss = 0.02728245\n",
      "Iteration 19, loss = 0.03045743\n",
      "Iteration 20, loss = 0.03104707\n",
      "Iteration 21, loss = 0.03074825\n",
      "Iteration 22, loss = 0.02201815\n",
      "Iteration 23, loss = 0.03362259\n",
      "Iteration 24, loss = 0.03068065\n",
      "Iteration 25, loss = 0.02587434\n",
      "Iteration 26, loss = 0.02185822\n",
      "Iteration 27, loss = 0.01966379\n",
      "Iteration 28, loss = 0.02234761\n",
      "Iteration 29, loss = 0.01922309\n",
      "Iteration 30, loss = 0.02040709\n",
      "Iteration 31, loss = 0.02275785\n",
      "Iteration 32, loss = 0.03246280\n",
      "Iteration 33, loss = 0.02110143\n",
      "Iteration 34, loss = 0.01883724\n",
      "Iteration 35, loss = 0.02388621\n",
      "Iteration 36, loss = 0.01624014\n",
      "Iteration 37, loss = 0.01731902\n",
      "Iteration 38, loss = 0.02139497\n",
      "Iteration 39, loss = 0.02561459\n",
      "Iteration 40, loss = 0.02117299\n",
      "Iteration 41, loss = 0.01878830\n",
      "Iteration 42, loss = 0.01158340\n",
      "Iteration 43, loss = 0.01956093\n",
      "Iteration 44, loss = 0.02066182\n",
      "Iteration 45, loss = 0.01979342\n",
      "Iteration 46, loss = 0.02363753\n",
      "Iteration 47, loss = 0.02367547\n",
      "Iteration 48, loss = 0.02082848\n",
      "Iteration 49, loss = 0.01460321\n",
      "Iteration 50, loss = 0.01306188\n",
      "Iteration 51, loss = 0.01226877\n",
      "Iteration 52, loss = 0.01139973\n",
      "Iteration 53, loss = 0.01127147\n",
      "Iteration 54, loss = 0.01584698\n",
      "Iteration 55, loss = 0.01768233\n",
      "Iteration 56, loss = 0.02251928\n",
      "Iteration 57, loss = 0.01998125\n",
      "Iteration 58, loss = 0.01653148\n",
      "Iteration 59, loss = 0.00795160\n",
      "Iteration 60, loss = 0.01904340\n",
      "Iteration 61, loss = 0.01213132\n",
      "Iteration 62, loss = 0.01184418\n",
      "Iteration 63, loss = 0.01083713\n",
      "Iteration 64, loss = 0.01601322\n",
      "Iteration 65, loss = 0.01440927\n",
      "Iteration 66, loss = 0.01560925\n",
      "Iteration 67, loss = 0.01627927\n",
      "Iteration 68, loss = 0.00685662\n",
      "Iteration 69, loss = 0.01786066\n",
      "Iteration 70, loss = 0.02787242\n",
      "Iteration 71, loss = 0.02142081\n",
      "Iteration 72, loss = 0.01277027\n",
      "Iteration 73, loss = 0.01030001\n",
      "Iteration 74, loss = 0.00924915\n",
      "Iteration 75, loss = 0.00804732\n",
      "Iteration 76, loss = 0.01103772\n",
      "Iteration 77, loss = 0.01377156\n",
      "Iteration 78, loss = 0.01871863\n",
      "Iteration 79, loss = 0.01061368\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "print(\"ADAM Classifier\")\n",
    "start_time = time.time()\n",
    "Classifier_ADAM = MLPClassifier(hidden_layer_sizes =  hidden_layer_s, solver = \"adam\", verbose=True).fit(training_data, training_target)\n",
    "training_times[2] = time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
